{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"name":"Week 8-2 Building Chatbot with NLP.ipynb","provenance":[{"file_id":"10ZkxcL2AeSZ7J6OD5NaVXO4oq55Kc9Oo","timestamp":1633531818986}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"stylish-piece"},"source":["# Week 8-2.NLP를 이용한 Chatbot 개발 실습"],"id":"stylish-piece"},{"cell_type":"markdown","metadata":{"id":"exact-special"},"source":["## 1.Bag of Words Model\n","\n","* 기계는 인간처럼 언어를 이해하지 못함.\n","* 언어를 숫자로 치환해서 처리해야 함. 따라서 사람이 말하는 \"이해\"와는 거리가 있음.\n","* BoW는 자연어처리에서 가장 많이 활용되는 워드벡터 모델.\n","* 문장에서 단어가 출현하는 순서 등을 고려하지 않고 특정 문장(도큐멘트)에 단어가 존재하는지 여부만 확인함.\n","\n","* 예:\n","    * D1: John is quicker than Mary. \n","    * D2: Mary is quicker than John. \n","    * Terms `=> [ John , is , Mary , quicker , than ]`\n","    \n","|    \t| john \t| is \t| mary \t| quicker \t| than \t|\n","|:--\t|:----:\t|:--:\t|:----:\t|:-------:\t|:----:\t|\n","| D1 \t|   1  \t|  1 \t|   0  \t|    1    \t|   1  \t|\n","| D2 \t|   0  \t|  1 \t|   1  \t|    1    \t|   1  \t|"],"id":"exact-special"},{"cell_type":"markdown","metadata":{"id":"distinguished-wrapping"},"source":["## 2.TF-IDF\n","\n","* **TF(단어 빈도, term frequency)는 특정한 단어가 문서 내에 얼마나 자주 등장하는지를 나타내는 값. 이 값이 높을수록 특정한 단어가 문서에서 중요하다고 생각할 수 있다.**\n","* **하지만 하나의 문서에서만 자주 등장하는 것이 아니라 여러 문서에서 공통적으로 자주 등장하면 단어의 중요도는 낮아진다. (예: a, the, of, is 등)**\n","* DF(문서 빈도, document frequency)라고 하며, 이 값의 역수를 IDF(역문서 빈도, inverse document frequency)라고 한다.\n","* TF-IDF는 TF와 IDF를 곱한 값으로 점수가 높은 단어일수록 다른 문서에는 많지 않고 해당 문서에서 자주 등장하는 단어를 의미한다."],"id":"distinguished-wrapping"},{"cell_type":"markdown","metadata":{"id":"mighty-hindu"},"source":["### Term Frequency (tf)\n","\n","* 문서에서 해당 단어가 얼마나 나타났는가?를 의미한다. 예를 들어 문서에 “국민”이라는 단어가 10번 나오면 TF값은 10이 된다."],"id":"mighty-hindu"},{"cell_type":"markdown","metadata":{"id":"progressive-angel"},"source":["### Document Frequency (df)\n","\n","* DF는 전체 문서들에서 몇개의 문서에 해당 단어가 나타나있는지에 대한 값. \n","    * DF = 해당 단어가 나타난 문서수 / 전체 문서 수\n"],"id":"progressive-angel"},{"cell_type":"markdown","metadata":{"id":"quick-guinea"},"source":["### Inverse Document Frequency (idf)\n","\n","* IDF는 DF의 역수이다. 가장 DF가 큰 값을 1로 만들기 위해 IDF를 사용한다.\n","    * IDF = 전체 문서 수 / 해당 단어가 나타난 문서 수\n","* 그러나 보통 값이 크게 나타나기 때문에 log를 씌워 계산한다.\n","    * IDF = log(전체 문서 수 / 해당 단어가 나타난 문서수)"],"id":"quick-guinea"},{"cell_type":"markdown","metadata":{"id":"private-basin"},"source":["### TF-IDF를 계산하는 과정\n","\n","* TF값과 IDF값을 곱한 값. \n","\n","\n","* D1 = \"Tom plays soccer.\"\n","* D2 = \"Tom loves soccer and baseball.\"\n","* D3 = \"baseball is his hobby and his job.\"\n","\n","\n","`BoW => [Tom, plays, soccer, loves, and, baseball, is, his, hobby, job]`"],"id":"private-basin"},{"cell_type":"markdown","metadata":{"id":"mathematical-ottawa"},"source":["### TF의 계산\n","* 각 문서에 출현한 단어의 숫자를 카운트\n","\n","|    \t| Tom \t| plays\t|soccer | loves \t| and \t| baseball \t| is \t| his \t| hobby \t| job \t|\n","|:--\t|:----:\t|:--:\t|:----:\t|:-------:\t|:----:\t|:----:\t|:----:\t|:----:\t|:----:\t|:----:\t|\n","| D1 \t|   1  \t|  1 \t|   1  \t|    0    \t|   0  \t|   0  \t|   0  \t|   0  \t|   0  \t|   0  \t|\n","| D2 \t|   1  \t|  0 \t|   1  \t|    1    \t|   1  \t|   1  \t|   0  \t|   0  \t|   0  \t|   0  \t|\n","| D3 \t|   0  \t|  0 \t|   0  \t|    0    \t|   1  \t|   1  \t|   1  \t|   2  \t|   1  \t|   1  \t|"],"id":"mathematical-ottawa"},{"cell_type":"markdown","metadata":{"id":"hazardous-format"},"source":["### IDF의 계산\n","\n","* IDF값은 log(전체 문서 수 / 해당 단어가 나타난 문서수)\n","* Tom을 계산해보면 해당 단어가 나타난 문서수는 2이고 현재 전체 문서의 수는 3이므로 log(3/2) ≒ 0.18\n","    * 즉, 한 단어가 여러 문서에 출현할수록 한 문서에서 그 단어의 중요도는 떨어짐\n","    \n","|    \t| Tom \t| plays\t|soccer | loves \t| and \t| baseball \t| is \t| his \t| hobby \t| job \t|\n","|:--\t|:----:\t|:--:\t|:----:\t|:-------:\t|:----:\t|:----:\t|:----:\t|:----:\t|:----:\t|:----:\t|\n","| D1 \t|   0.18  \t|  0.48 \t|   0.18  \t|    0    \t|   0  \t|   0  \t|   0  \t|   0  \t|   0  \t|   0  \t|\n","| D2 \t|   0.18  \t|  0 \t|   0.18  \t|    0.48    \t|   0.18  \t|   0.18  \t|   0  \t|   0  \t|   0  \t|   0  \t|\n","    | D3 \t|   0  \t|  0 \t|   0  \t|    0    \t|   0.18  \t|   0.18  \t|   0.48  \t|   0.48  \t|   0.48  \t|   0.48  \t|"],"id":"hazardous-format"},{"cell_type":"markdown","metadata":{"id":"induced-diving"},"source":["### TF-IDF의 계산\n","\n","* TF-IDF값은 TF값과 IDF값을 곱한 값. \n","* ‘his’ 단어의 경우 TF의 값이 2, IDF값이 0.48이므로 두개를 곱한 2 x 0.48 = 0.96이 TF-IDF값이 된다. \n","* 아래 표를 보면 D1에서는 ‘plays’, D2에서는 ‘loves’, D3에서는 ‘his’가 가장 중요한 단어가 된다.\n","\n","|    \t| Tom \t| plays\t|soccer | loves \t| and \t| baseball \t| is \t| his \t| hobby \t| job \t|\n","|:--\t|:----:\t|:--:\t|:----:\t|:-------:\t|:----:\t|:----:\t|:----:\t|:----:\t|:----:\t|:----:\t|\n","| D1 \t|   0.18  \t|  0.48 \t|   0.18  \t|    0    \t|   0  \t|   0  \t|   0  \t|   0  \t|   0  \t|   0  \t|\n","| D2 \t|   0.18  \t|  0 \t|   0.18  \t|    0.48    \t|   0.18  \t|   0.18  \t|   0  \t|   0  \t|   0  \t|   0  \t|\n","    | D3 \t|   0  \t|  0 \t|   0  \t|    0    \t|   0.18  \t|   0.18  \t|   0.48  \t|   0.96  \t|   0.48  \t|   0.48  \t|"],"id":"induced-diving"},{"cell_type":"markdown","metadata":{"id":"difficult-nitrogen"},"source":["## 3.TF-IDF 계산 실습\n","\n","다음의 도큐멘트를 이용하여 TF-IDF 를 만들어 보자."],"id":"difficult-nitrogen"},{"cell_type":"code","metadata":{"id":"reasonable-obligation"},"source":["docA = \"The cat sat on my face\"\n","docB = \"The dog sat on my bed\""],"id":"reasonable-obligation","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sunset-anatomy"},"source":["### Bag of Words (BoW)"],"id":"sunset-anatomy"},{"cell_type":"code","metadata":{"id":"passive-stick"},"source":["bowA = docA.split(\" \")\n","bowB = docB.split(\" \")"],"id":"passive-stick","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"disturbed-alexander"},"source":["print(bowA)\n","print(bowB)"],"id":"disturbed-alexander","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"invalid-camera"},"source":["두개의 bow를 합쳐 하나의 set을 만든다. (단어의 유니크한 리스트 == 모든 도큐멘트에 출현한 단어의 사전)"],"id":"invalid-camera"},{"cell_type":"code","metadata":{"id":"respiratory-ottawa"},"source":["wordSet = set(bowA).union(set(bowB))"],"id":"respiratory-ottawa","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"patent-indie"},"source":["print(wordSet)"],"id":"patent-indie","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"meaning-adrian"},"source":["각 문서에서 bow의 단어가 출현한 빈도를 계산(TF)하기 위해서 문서 당 bow의 dictionary를 만든다.\n","\n","아직은 카운트를 하기 전이기 때문에 빈도수는 0으로 설정하도록 하자."],"id":"meaning-adrian"},{"cell_type":"code","metadata":{"id":"intense-alarm"},"source":["wordDictA = dict.fromkeys(wordSet, 0) \n","wordDictB = dict.fromkeys(wordSet, 0) "],"id":"intense-alarm","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"swiss-laugh"},"source":["print(wordDictA)\n","print(wordDictB)"],"id":"swiss-laugh","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"recorded-rebound"},"source":["### TF의 계산\n","\n","각 문서 별로 단어가 출현한 빈도를 계산"],"id":"recorded-rebound"},{"cell_type":"code","metadata":{"id":"diverse-grade"},"source":["for word in bowA:\n","    wordDictA[word]+=1\n","    \n","for word in bowB:\n","    wordDictB[word]+=1"],"id":"diverse-grade","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"honest-monthly"},"source":["print(docA)\n","print(wordDictA)\n","print(docB)\n","print(wordDictB)"],"id":"honest-monthly","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"virtual-protein"},"source":["import pandas as pd\n","pd.DataFrame([wordDictA, wordDictB])"],"id":"virtual-protein","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"clear-sister"},"source":["def computeTF(wordDict, bow):\n","    tfDict = {}\n","    bowCount = len(bow)\n","    for word, count in wordDict.items():\n","        tfDict[word] = count/float(bowCount) # 각 문장의 전체 단어에서(bowCount) 각 단어가 출현한 비율을 계산. (여기서는 1/6) \n","    return tfDict"],"id":"clear-sister","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"documentary-parking"},"source":["tfBowA = computeTF(wordDictA, bowA)\n","tfBowB = computeTF(wordDictB, bowB)"],"id":"documentary-parking","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sustained-headquarters"},"source":["tfBowA"],"id":"sustained-headquarters","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"excellent-today"},"source":["tfBowB"],"id":"excellent-today","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"backed-volume"},"source":["### IDF의 계산\n","\n","IDF = log(전체 문서 수 / 해당 단어가 나타난 문서수)"],"id":"backed-volume"},{"cell_type":"code","metadata":{"id":"reasonable-compiler"},"source":["def computeIDF(docList):\n","    import math\n","    idfDict = {}\n","    N = len(docList) # 전체 문서의 갯수\n","    \n","    idfDict = dict.fromkeys(docList[0].keys(), 0) # empty dictionary with 0\n","    for doc in docList:\n","        #{'face': 1, 'cat': 1, 'dog': 0, 'The': 1, 'sat': 1, 'my': 1, 'on': 1, 'bed': 0}\n","        for word, val in doc.items():\n","            if val > 0:\n","                idfDict[word] += 1 # 문서에 단어가 출현할 때마다 1 추가 (eg. idfDict['face'] += 1)\n","    \n","    for word, val in idfDict.items():\n","        idfDict[word] = math.log10(N / float(val))\n","        \n","    return idfDict "],"id":"reasonable-compiler","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"beginning-maintenance"},"source":["idfs = computeIDF([wordDictA, wordDictB])\n","print(idfs)"],"id":"beginning-maintenance","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"becoming-substitute"},"source":["### TF-IDF의 계산\n","\n","TF와 IDF를 곱한 값"],"id":"becoming-substitute"},{"cell_type":"code","metadata":{"id":"north-criminal"},"source":["def computeTFIDF(tfBow, idfs):\n","    tfidf = {}\n","    for word, val in tfBow.items():\n","        tfidf[word] = val*idfs[word]\n","    return tfidf"],"id":"north-criminal","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"subsequent-scroll"},"source":["tfidfBowA = computeTFIDF(tfBowA, idfs)\n","tfidfBowB = computeTFIDF(tfBowB, idfs)"],"id":"subsequent-scroll","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"spectacular-campaign"},"source":["print(tfidfBowA)\n","print(tfidfBowB)"],"id":"spectacular-campaign","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"compressed-peninsula"},"source":["import pandas as pd\n","pd.DataFrame([tfidfBowA, tfidfBowB])"],"id":"compressed-peninsula","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"exclusive-ottawa"},"source":["### SkLearn을 이용한 계산\n","\n","scikit-learn은 python을 대표하는 머신러닝 라이브러리. 다양한 기계학습 알고리즘이 구현되어 있음."],"id":"exclusive-ottawa"},{"cell_type":"code","metadata":{"id":"becoming-making"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd"],"id":"becoming-making","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"guilty-classification"},"source":["docA = \"The cat sat on my face\"\n","docB = \"The dog sat on my bed\""],"id":"guilty-classification","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"interracial-luxembourg"},"source":["tfidf = TfidfVectorizer()\n","response = tfidf.fit_transform([docA, docB])"],"id":"interracial-luxembourg","execution_count":null,"outputs":[]},{"cell_type":"code","source":["response.toarray()"],"metadata":{"id":"aB5WiojeIeUO"},"id":"aB5WiojeIeUO","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"urban-imagination"},"source":["`fit_transform()`은 문서의 tf-idf 값을 구해 sparce matrix 형태로 반환.\n","\n","`toarray()`를 이용하여 pandas의 dataframe 으로 변환가능"],"id":"urban-imagination"},{"cell_type":"code","metadata":{"id":"guilty-gilbert"},"source":["df = pd.DataFrame(response.toarray())\n","feature_names = tfidf.get_feature_names()\n","df.columns = feature_names\n","df.index = [\"docA\", \"docB\"]"],"id":"guilty-gilbert","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"assigned-romania"},"source":["df"],"id":"assigned-romania","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"proved-notice"},"source":["## 4.Cosine Similarity (코사인 유사도)\n","\n","코사인 유사도는 벡터간의 코사인 각도를 이용하여 서로간에 얼마나 유사한지를 산정. \n","\n","각도를 유사도로 판별하기 때문에 거리가 중요하지 않을 경우 사용되는 방식"],"id":"proved-notice"},{"cell_type":"markdown","metadata":{"id":"precise-participant"},"source":["### 코사인 유사도 공식\n","\n","* 코사인 유사도는 -1 ~ 1 사이의 값을 가짐. \n","* 벡터들의 방향이 완전히 다를 경우, 즉 각도가 180°일 경우에는 -1\n","* 방향이 완전히 동일하면 1\n","* 값이 90°의 각일 경우 0\n","\n","![alt text](https://images.deepai.org/glossary-terms/cosine-similarity-1007790.jpg \"Cosine Similarity\")\n","https://deepai.org/machine-learning-glossary-and-terms/cosine-similarity\n","\n","![alt text](https://images.deepai.org/glossary-terms/cosine-similarity-4063640.jpg \"Cosine Similarity Equation\")\n","\n"],"id":"precise-participant"},{"cell_type":"code","metadata":{"id":"funny-rendering"},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity"],"id":"funny-rendering","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"professional-michigan"},"source":["docA = \"The cat sat on my face\"\n","docB = \"The dog sat on my bed\"\n","docC = \"I love my flight hanging over my bed\""],"id":"professional-michigan","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"handed-heavy"},"source":["tfidf = TfidfVectorizer()\n","response = tfidf.fit_transform([docA, docB, docC])"],"id":"handed-heavy","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fatal-estate"},"source":["df = pd.DataFrame(response.toarray())\n","feature_names = tfidf.get_feature_names()\n","df.columns = feature_names\n","df.index = [\"docA\", \"docB\", \"docC\"]"],"id":"fatal-estate","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"caring-multimedia"},"source":["df"],"id":"caring-multimedia","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"satellite-ribbon"},"source":["# query = \"a dog on my bed\"\n","query = \"a flight over the bed\"\n","query_vec = tfidf.transform([query]) #계산된 IDF을 이용하여 query의 TF-IDF 계산"],"id":"satellite-ribbon","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"elder-humanitarian"},"source":["results = cosine_similarity(response, query_vec)"],"id":"elder-humanitarian","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"british-greece"},"source":["results # [A, B, C] 의 cosine similarity"],"id":"british-greece","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3808f11c"},"source":["## 5.TF-IDF 로 챗봇 만들기"],"id":"3808f11c"},{"cell_type":"markdown","metadata":{"id":"249c449a"},"source":["### 가장 심플한 in-out 모델\n","\n","* 입력을 받으면 (greet_in) DB(greet_out)에서 무작위로 하나를 선택하여 반환"],"id":"249c449a"},{"cell_type":"code","metadata":{"id":"inappropriate-hardwood"},"source":["import csv\n","import random\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from nltk.stem.porter import PorterStemmer"],"id":"inappropriate-hardwood","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d9ca09a7"},"source":["greet_in = ('hey', 'sup', 'waddup', 'wassup', 'hi', 'hello','ola', 'bonjour', 'namastay', 'hola', \n","            'heya', 'hiya', 'howdy', 'greetings', 'yo', 'ahoy')\n","\n","greet_out = ['hey', 'hello', 'hi there', 'hi', 'heya', 'hiya', 'howdy', 'greetings', '*nods*', 'ola', \n","             'bonjour', 'namastay']\n","\n","def greeting(sent):\n","    for word in sent.split():\n","        if word.lower() in greet_in:\n","            return random.choice(greet_out)"],"id":"d9ca09a7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ff04a83"},"source":["print(greeting(\"hello everyone\"))"],"id":"6ff04a83","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"korean-prisoner"},"source":["### TF-IDF 를 이용하여 입력된 질문과 가장 가까운 문장을 탐색하는 챗봇"],"id":"korean-prisoner"},{"cell_type":"code","metadata":{"id":"cbb8c67d"},"source":["import csv\n","import random\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from nltk.stem.porter import PorterStemmer"],"id":"cbb8c67d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"deadly-catalyst"},"source":["### 채팅 답변을 위한 데이터베이스 구축\n","* small_talk_responses에는 질문의 예와 그에 대한 답변이 dictionary 형태로 저장 \n","    * key->질문, value->답변\n","* dictionary의 value 만 뽑아서 response에 저장"],"id":"deadly-catalyst"},{"cell_type":"code","metadata":{"id":"448e761c"},"source":["small_talk_responses = {\n","    'how are you': 'I am fine. Thank you for asking ',\n","    'how are you doing': 'I am fine. Thank you for asking ',\n","    'how do you do': 'I am great. Thanks for asking ',\n","    'how are you holding up': 'I am fine. Thank you for asking ',\n","    'how is it going': 'It is going great. Thank you for asking ',\n","    'goodmorning': 'Good Morning ',\n","    'goodafternoon': 'Good Afternoon ',\n","    'goodevening': 'Good Evening ',\n","    'good day': 'Good day to you too ',\n","    'whats up': 'The sky ',\n","    'sup': 'The sky ',\n","    'thanks': 'Dont mention it. You are welcome ',\n","    'thankyou': 'Dont mention it. You are welcome ',\n","    'thank you': 'Dont mention it. You are welcome ',\n","    'i am tired': 'please take some rest'\n","}\n","\n","response = small_talk_responses.values()\n","response = [str (item) for item in response]"],"id":"448e761c","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b29e036c"},"source":["response"],"id":"b29e036c","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"minimal-standing"},"source":["### TF-IDF와 Cosine Similarity 를 이용한 답변 찾기\n","\n","* `tfidf_cosim_smalltalk`method는 두개의 패러미터를 가짐\n","    * doc: small_talk_responses\n","    * query: 사용자가 입력한 질문"],"id":"minimal-standing"},{"cell_type":"code","metadata":{"id":"34f83647"},"source":["def tfidf_cosim_smalltalk(doc, query):\n","    query = [query]\n","    tf = TfidfVectorizer(use_idf=True, sublinear_tf=True)\n","    tf_doc = tf.fit_transform(doc)\n","    tf_query = tf.transform(query)\n","    results = cosine_similarity(tf_doc,tf_query).flatten()\n","    related_docs_indices = results.argsort()[:-2:-1] # select the last element :-2:-1\n","    # print(results.argsort())\n","    \n","    if (results[related_docs_indices] > 0.7):\n","        ans = [response[i] for i in related_docs_indices[:1]]\n","        return ans[0]"],"id":"34f83647","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"naval-porcelain"},"source":["print(tfidf_cosim_smalltalk(small_talk_responses, \"how are you\"))"],"id":"naval-porcelain","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"confused-recognition"},"source":["### 챗봇 완성"],"id":"confused-recognition"},{"cell_type":"code","metadata":{"id":"tutorial-petroleum"},"source":["stop = True\n","while stop == True :\n","    \n","    name = input('\\nHello, my name is SimpleChat. What is your name? : ')\n","    name = name.lower()\n","    \n","    stop1 = True\n","    while stop1==True:\n","        query = input('\\nHi ' + name + ', How can I help you? If you want to exit, type Bye. : ')\n","        query = query.lower()\n","        query = query.strip(\"!@#$%^&*()<>,;?\")\n","        print(query)\n","        \n","        if query == 'bye':\n","            stop1 = False\n","            print('\\nSimpleChat: Bye, take care, ' + name + '.')                \n","    \n","        else:\n","            if greeting(query) != None:\n","                print('\\nSimpleChat: ' + greeting(query) + ', ' + name + '.')\n","            elif tfidf_cosim_smalltalk(small_talk_responses, query) != None:\n","                x = tfidf_cosim_smalltalk(small_talk_responses, query)\n","                print('\\nSimpleChat: ' + x + ', ' + name + '.')\n","                \n","    stop = False"],"id":"tutorial-petroleum","execution_count":null,"outputs":[]}]}