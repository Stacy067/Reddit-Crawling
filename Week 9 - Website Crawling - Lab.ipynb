{"cells":[{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"zDPCMAbug09u"},"source":["# Week 9.Website Crawling"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"cNZrh-Zqg09x"},"source":["## 1.BeautifleSoup4 를 이용한 웹페이지 크롤링"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"x_Yjo2PKg09y"},"source":["### 1.1 BeautifulSoup 모듈을 로딩한다."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"6hm8tdlsg09z"},"outputs":[],"source":["from bs4 import BeautifulSoup"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"1G46-lA-g090"},"source":["아래의 텍스트 라인은 샘플 HTML 문서이다. 실제로는 저장된 HTML파일을 불러오거나 웹에서 바로 HTML문서를 다운로드 받아 데이터 처리를 한다."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"hSwWq9BAg090"},"outputs":[],"source":["html_doc = \"<html><body><h1>Mr. Belvedere Fan Club</h1><div id='nav'>navigation bar</div><div class='nav'>navigation class</div><div class='header'><a href='twitter_anywhere'>my twitter</a></div></body></html>\""]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"Cc5ZU7cRg091"},"source":["BeautifulSoup의 \"html.parser\"를 이용하여 문서를 parsing한다. \"html.parser\" 외에 \"xml\", \"html5lib\" 등의 parser를 제공한다.\n","https://www.crummy.com/software/BeautifulSoup/bs4/doc/"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"iC3xhSpdg092"},"outputs":[],"source":["soup = BeautifulSoup(html_doc, \"html.parser\")\n","soup"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"5er0TKCkg093"},"outputs":[],"source":["print(type(soup))"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"vhRBkQdyg094"},"source":["parsing된 HTML 문서를 보기 좋게 보여준다.\n","\n","range를 사용하여 원하는 만큼만 볼 수 있다. `soup.prettify()[0:100]`"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"AzPkxAuOg095"},"outputs":[],"source":["print(soup.prettify())"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"bis0bxLTg095"},"source":["전체 문서에서 ```<h1>``` 인 요소들만 찾아 list로 반환한다."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"AqNgxljRg096"},"outputs":[],"source":["heading = soup.find_all(\"h1\")\n","heading"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"ShaN-LUSg096"},"source":["요소의 내용(text)를 반환하기 위해서 `get_text()` 혹은 `text`를 사용한다. "]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"THX8L--0g096"},"outputs":[],"source":["heading[0].get_text()"]},{"cell_type":"code","source":["heading[0].text"],"metadata":{"id":"sMXsKQoFkPkt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"bHychAiwg097"},"source":["<질문> ```heading.get_text()``` 는 에러가 나는 이유는?"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"PknUYiz9g097"},"outputs":[],"source":["divs = soup.find_all(\"div\")\n","divs"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"Pj4eUiq7g098"},"source":["여러 요소 중, class와 id 등으로 filtering 하기 위해서 두번째 패러미터를 사용한다."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"zhnBKVmIg098"},"outputs":[],"source":["divs = soup.find_all(\"div\", class_=\"nav\")\n","divs"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"RsMmC_uPg098"},"outputs":[],"source":["divs = soup.find_all(\"div\", id=\"nav\")\n","divs"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"ZZqt6Gl9g099"},"outputs":[],"source":["id_list = []\n","divs = soup.find_all(\"div\", class_=\"header\")\n","for div in divs:\n","    if div.a[\"href\"] == \"twitter_anywhere\":\n","        id_list.append(div.a.text) # text 는 get_text()와 동일하게 사용됨.\n","id_list"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"p_NnEANYg099"},"source":["## 2.twitter 아이디와 사용자 이름 수집\n","\n","twtkr_example.html 파일을 읽어 트위터 아이디와 사용자 이름을 수집해 보자. 수집된 id 에서 @ 기호를 삭제하여 출력한다.\n","* 예: u_simin, 유시민"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"P9i4LwVOlVRP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"M63AcylRg09-"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","\n","html_doc = \"\"\n","with open(\"/content/drive/MyDrive/_SocialComp_2022-Lab/Week9/data/twtkr_example.html\") as file:\n","    html_doc = file.read()\n","\n","soup = BeautifulSoup(html_doc, \"html.parser\")"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"adUcE1fYg09-"},"outputs":[],"source":["divs = soup.find_all(\"div\", class_=\"header\")\n","for div in divs:\n","    print(div.cite.a[\"href\"].replace(\"/\", \"\"), div.cite.a.text)"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"wezDi659g09_"},"source":["## 3.URL 가져와서 데이터 수집"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"sfzptdW2g09_"},"outputs":[],"source":["import urllib.request\n","with urllib.request.urlopen(\"https://media.daum.net/\") as url:\n","    doc = url.read()\n","    soup = BeautifulSoup(doc, \"html.parser\")\n","    strongs = soup.find_all(\"strong\", class_=\"tit_g\")\n","    for strong in strongs:\n","        print(strong)\n","        print(strong.a.text)\n","        print(strong.a[\"href\"])\n","        break"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"Ea3MQiJAg09_"},"source":["### 실습 1: 데이터 수집을 위한 리스트 작성\n","\n","위의 주소에서 수집하고자 하는 URL의 리스트를 작성해보자."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"_r6cAXoKg09_"},"outputs":[],"source":["import urllib.request\n","\n","url_list = []\n","with urllib.request.urlopen(\"https://media.daum.net/\") as url:\n","    doc = url.read()\n","    soup = BeautifulSoup(doc, \"html.parser\")\n","\n","        \n","url_list"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"KFD0KVqhg0-A"},"source":["### 실습 2: 기사 수집 1\n","\n","#### 주어진 URL의 기사의 타이틀을 수집해보자."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"W6n8fd1ng0-A"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import urllib.request\n","\n","base_url = \"https://v.daum.net/v/20220424121601543\"\n","\n","with urllib.request.urlopen(base_url) as url:\n","    doc = url.read()\n","    soup = BeautifulSoup(doc, \"html.parser\")\n","\n"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"hRc-vbaUg0-A"},"source":["#### 주어진 기사의 본문을 수집해 보자.\n","\n","이 기사에는 figurecaption 부분이 있는데, 다음과 같은 코드로 제외한다. \n","\n","`unwanted = div.find(\"p\", class_=\"link_figure\")\n","unwanted.extract()`"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"byuJNMbmg0-B"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import urllib.request\n","\n","base_url = \"https://v.daum.net/v/20220424121601543\"\n","\n","with urllib.request.urlopen(base_url) as url:\n","    doc = url.read()\n","    soup = BeautifulSoup(doc, \"html.parser\")\n"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"77r7jQ67g0-B"},"source":["#### 기자이름과 입력 날짜를 수집해보자."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"uMpca0Nhg0-B"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import urllib.request\n","from datetime import datetime as dt\n","\n","base_url = \"https://v.daum.net/v/20220424121601543\"\n","\n","with urllib.request.urlopen(base_url) as url:\n","    doc = url.read()\n","    soup = BeautifulSoup(doc, \"html.parser\")\n"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"wQr3WK4Bg0-C"},"source":["#### 언론사 이름을 수집해보자."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"7zjSYDiOg0-C"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import urllib.request\n","\n","base_url = \"https://v.daum.net/v/20220424121601543\"\n","\n","with urllib.request.urlopen(base_url) as url:\n","    doc = url.read()\n","    soup = BeautifulSoup(doc, \"html.parser\")\n"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"1_NG5GjNg0-C"},"source":["### 실습 3: 기사 수집 2\n","\n","#### 위의 내용을 바탕으로 메인에 노출된 뉴스 전체를 수집해서 JSON으로 저장해 보자.\n","1. 위의 코드를 합쳐서\n","    - 기사의 리스트 수집\n","    - 각 기사 내용 수집\n","2. JSON 저장"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"HSe3ceDwg0-D"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import urllib.request\n","from datetime import datetime as dt\n","\n"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"wQRqk8Y7g0-D"},"source":["### 3.1 JSON 파일 만들기\n","\n","```\n","import json\n","from collections import OrderedDict\n"," \n","# Ready for data\n","group_data = OrderedDict()\n","items = OrderedDict()\n"," \n","group_data[\"url\"] = \"http://......\"\n","group_data[\"title\"] = \"article_title\"\n"," \n","items[\"item1\"] = \"item_name1\"\n","items[\"item2\"] = \"item_name2\"\n","items[\"item3\"] = \"item_name3\"\n"," \n","group_data[\"items\"] = items\n"," \n","# Print JSON\n","print(json.dumps(group_data, ensure_ascii=False, indent=\"\\t\") )\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"8dXWVkRSg0-D"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import urllib.request\n","import json\n","from collections import OrderedDict\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"BRuhyuugg0-E"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Week 9 - Website Crawling - Lab.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}