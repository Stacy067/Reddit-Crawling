{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Week 7 NLP Basic.ipynb","provenance":[{"file_id":"1mq86skIku55u2wcOxF1O81Nty3A2ksg0","timestamp":1633526436112}],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","source":["# Week 7. Natural Language Processing Basic"],"metadata":{"id":"qqjJ9dDg-BXl"}},{"cell_type":"markdown","metadata":{"id":"SX-DOo-6uOIa"},"source":["# 1.Natural Language Processing (자연어처리)"]},{"cell_type":"markdown","metadata":{"id":"imperial-voluntary"},"source":["## 1.1 NLP(Natural Language Processing) 란 무엇인가?\n","\n","* 자연어(Natural Language)란 우리가 일상에서 사용하는 언어를 지칭\n","\n","* 자연어처리(Natural Language Processing)을 한다는 것은 우리의 일상 언어(=텍스트 데이터)로 부터 의미있는 무엇인가를 추출하고 분석하는 작업\n","\n","* NLP는 크게 자연어이해(Natural Language Understanding)과 자연어생성(Natural Language Generation)으로 나뉘는데, 챗봇을 개발하는데에는 보통 두가지 기술이 모두 사용됨 \n","\n","* 그러나 본 실습에서는 NLG는 다루지 않고 현재의 NLG 접근의 기술적 한계를 설명할 예정임"]},{"cell_type":"markdown","metadata":{"id":"copyrighted-athens"},"source":["## 1.2 NLP가 사용되는 분야\n","\n","\n","* **텍스트 분류(Text Classification)**: 입력된 텍스트가 어떤 범주에 속하는지 판단하는 작업이다. 전통적으로 텍스트 분류는 스팸(spam) 메일인지 여부를 판단하는데 사용되었다. 스팸 메일 분류는 메일을 크게 스팸 메일과 일반 메일로 레이블링하여 학습한 후 새로 입력된 텍스트가 스팸 메일과 일반 메일 중 어느 범주에 속하는지를 학습된 모형을 바탕으로 판단한다. 전통적인 자연어처리 기법에서는 스팸 메일을 구성하는 특징(feature)을 추출하고 이를 바탕으로 학습된 모형을 구축하였으나 최근 신경망모형 기반의 자연어처리 분야에서는 이 과정이 혁신적으로 개선되어 그 특징을 자동으로 찾아내기도 한다. 감성분석 등의 분류 작업도 텍스트 분류에 속한다.\n","\n","* **감성 분석(Sentiment Analysis)**: 텍스트 분류 기법의 하나로 텍스트에 포함된 의견이나 감성 등을 분석한다. 주로 영화평 분석, 고객의 의견 분석, 정치관련 유권자 메시지 분석 등 사람의 눈으로 일일이 읽기 어려울 정도로 많은 양의 데이터를 처리해야 할 경우에 유용하며 감정가(valence)를 찾아내는 작업이다. 다음은 나이브 베이지언(Naive Bayesian) 방법으로 분류한 네이버 영화평의 감성 분석 사례이다.\n","\n","| 영화 평 | 분류결과 |\n","|------|--------|\n","|   올해 영화관에서 본 영화중 최고였다매혹적이고 아름답고 슬프기도하다   |    Positive    |\n","|   진지하고 여운이 남는 영화에요 추천합니다   |    Positive    |\n","|   재미 드럽게 없음..포와로역 외모도 안어울리고, 지루하고, 긴장감도 없고... 실망입니다. 졸려요..   |    Negative    |\n","|   평소에 잠을 푹 못잤는데 심야로 보다가 아주 잘 자고갑니다 너무너무고마운영화 불면증치료에 좋은 힐링   |    Negative    |\n","나이브 베이지언(Naive Bayesian) 방법으로 분류한 네이버 영화평의 감성 분석 사례\n","\n","* **텍스트 요약(Summarization)**: 문서에서 중요한 내용을 간추려 작은 요약문을 만드는 것이다. 많은 문서를 읽고 이해하는 과정은 시간이 많이 걸리기 때문에 텍스트 요약은 문서를 이해하는 시간을 줄여줄 수 있다. 텍스트를 요약하는 기법은 문장을 생성하는 기법에 따라 크게 추출적(extractive) 요약과 추상적(abstractive) 요약으로 나눈다. 추출적 요약은 문서 내에서 핵심 단어와 문장을 뽑고 이를 연결하여 요약문을 만드는 방법이다. 반면 추상적 요약은 문서의 내용을 잘 설명할 수 있도록 핵심 개념을 바탕으로 문장을 생성해내는 방법이다. https://quillbot.com/\n","\n","* **의미연결망 분석(Semantic Network Analysis)**: 하나의 문장, 혹은 문서에서 사용된 단어는 같은 의미를 설명하기 위해 사용되었을 가능성이 높다. 이 점을 바탕으로 문서 내 단어의 네트워크를 구성하여 단어 간의 관계성을 파악할 수 있는데 이와 관련된 분석기법이 의미연결망 분석(Semantic Network Analysis)이다. 이 방법은 주로 메시지 등을 분석할 때 특정 키워드가 내포하는 의미를 확장하여 살펴보는데 유용하다. https://monot.github.io/election-2020/\n","\n","* **기계 번역(Translation):** 기계번역(Machine Translation)은 인공신경망 기반의 자연어처리 기법이 등장하면서 가장 많이 성장한 분야 중 하나이다. 구글, 네이버, 카카오 등 여러 회사에서 기계번역 알고리즘을 개발하여 서비스하고 있는데 방대한 문서의 학습을 통해 그 번역의 품질이 날이 갈수록 향상되고 있다.\n","\n","* **질의응답(Question Answering)**: 주어진 질의에 대한 답을 찾아 제시하는 연구분야이다. 미국 스탠포드 대학에서 기계독해 이해력 테스트를 위한 데이터셋인 SQuAD(Stanford Question Answering Dataset)을 제공하면서 이와 관련된 연구가 활발하게 진행되고 있다. 국내에서도 유사한 KorQuAD 데이터셋이 공개되었다. 이 데이터셋에 대한 실험결과를 보면  BERT(Bidirectional Encoder Representation of Transformer) 기반 신경망과 이들을 결한한 모형의 기계독해 능력이 인간보다 좋은 결과를 보이고 있지만 아직은 일반화하기 부족하다. 하지만 기계독해의 성능이 계속 향상되고 있어 조만간 무엇이든 물어봐도 대답해주는 인공지능이 등장할 가능성이 높다. https://rajpurkar.github.io/SQuAD-explorer/\n","\n","* **챗봇(Chatbot)**: 자연어처리를 바탕으로 사용자의 발화에서 숨은 의도(intent)를 찾아내고 자연스러운 인간의 언어로 답변을 해주는 서비스로 대화형 에이전트(Conversational Agent)라고 부른다. 최근 주요 기업들은 고객들과의 커뮤니케이션에 챗봇을 활용하고 있다. 챗봇의 주요 기술은 앞에서 설명한 것들로 이루어져 있다. 사용자의 발화를 분류하여 적절한 답변을 찾아내는 과정에는 텍스트 분류, 질의응답 등, 자연어처리의 많은 기술이 활용된다. 또한 사용자와의 라포(rapport) 형성을 위해서 감성 분류의 기술도 활용되고 있다.\n","\n","* **음성인식(Voice Recognition)**: 딥러닝(Deep Learning)과 텍스트 빅데이터의 등장으로 빠르게 성장하였다. 스마트폰에는 여러 IT 기업이 개발한 다양한 음성인식 기술이 도입되고 있으며 인공지능 스피커, 자동차, 티비 셋탑박스 등에도 음성인식 기술이 적용되고 있다. 음성인식기술은 발화된 내용을 인식하여 변환하는 기술이다. 따라서 음성인식 기술만으로는 효과적인 서비스를 구축하기 어려우며 자연스러운 의사소통이 이루어지기 위해서는 챗봇기술과 같이 사용자의 발화에서 의도(intent)를 찾아내는 기술이 결합되어야 한다. 자동통역 서비스와 같은 경우 음성인식 기술과 기계번역기술이 결합된 사례라고 볼 수 있다. 음성은 인간의 가장 자연스러운 대화의 방식이기 때문에 음성인식 기술의 개발 역시 활발하게 이루어지고 있다. "]},{"cell_type":"markdown","metadata":{"id":"fifty-tension"},"source":["## 1.3 일반적인 NLP 처리 프로세스\n","\n","1. 텍스트 전처리(Text Preprocessing): \n","- (영어의 경우) 대/소문자 변경, 어근 추출(stemming/lemmatization)\n","- (한글의 경우) 형태소 분석\n","- (공통) 특수문자 제거, 불용어(stopwords) 제거, 토큰화(tokenization), 이모티콘 삭제 등의 정규화 작업\n","\n","\n","2. 피처 벡터화 (Feature Vectorization): 텍스트를 벡터화하여 feature 를 추출. BoW(Bag of Words)와 Word2Vec 등이 사용됨. 최근에는 통계적 기법(TF-IDF, One-Hot Encoding 등) 대신 뉴럴네트웍기법(Word2Vec, BERT 등)이 많이 활용됨. Word Embedding이라고도 부름.\n","\n","3. 머신러닝 모델링: 벡터화된 데이터에 대한 모델을 만들고 학습/처리(예측)을 하는 단계"]},{"cell_type":"markdown","metadata":{"id":"determined-spectacular"},"source":["## 1.4 Tokenization\n","\n","* 텍스트를 처리하기 위해서는 분석의 기본단위로 단어를 사용\n","* 따라서 토큰화 작업이 필요\n","* 토큰화 작업 이후에는 어근추출, 형태소분석, 품사태깅 등의 작업이 필요\n","* 먼저 문장을 단어로 분리\n","    * John is quicker than Mary. -> `['John', 'is', 'quicker', 'than', 'Mary', '.']`\n","    * 꿈을 이루고자 하는 용기만 있다면 모든 꿈을 이룰 수 있다 -> `['꿈을', '이루고자', '하는', '용기만', '있다면', '모든', '꿈을', '이룰', '수', '있다']`\n","* 불용어 삭제 \n","    * `['John', 'is', 'quicker', 'than', 'Mary', '.']` -> `['John', 'quicker', 'Mary']`\n","    * a, an, the, of, to, be... 등이 불용어. 한글에서는 조사 등.\n","* 어근추출 혹은 형태소 분석\n","    * helps -> help\n","    * 질문이나 건의사항은 깃헙 이슈 트래커에 남겨주세요. -> `['질문', '건의', '건의사항', '사항', '깃헙', '이슈', '트래커']`\n","* 품사태깅 (KoNLPy의 예)\n","    * `>>> pprint(kkma.pos(u'오류보고는 실행환경, 에러메세지와함께 설명을 최대한상세히!^^'))\n","[(오류, NNG),\n"," (보고, NNG),\n"," (는, JX),\n"," (실행, NNG),\n"," (환경, NNG),\n"," (,, SP),\n"," (에러, NNG),\n"," (메세지, NNG),\n"," (와, JKM),\n"," (함께, MAG),\n"," (설명, NNG),\n"," (을, JKO),\n"," (최대한, NNG),\n"," (상세히, MAG),\n"," (!, SF),\n"," (^^, EMO)]`"]},{"cell_type":"markdown","metadata":{"id":"2vEDmLE8yDFc"},"source":["# 2.Text Preprocessing(텍스트 전처리)"]},{"cell_type":"markdown","metadata":{"id":"i5nEqz_WuOIe"},"source":["## 2.1. 토큰화 (Tokenization)\n","\n","텍스트 전처리과정에서 제일 먼저 수행하는 단계는 토큰화 단계. 이 단계에서는 문장을 단어 단위로 잘라낸다. 단어 토큰화를 하는 가장 쉬운 방법은 띄어쓰기(whitespace)를 중심으로 토큰을 만드는 것이다."]},{"cell_type":"code","metadata":{"id":"s0tUOhysuOIf"},"source":["MLKing = \"\"\"I have a dream that my four little children will one day live in a nation \n","            where they will not be judged by the color of their skin \n","            but by the content of their character.\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3JCpzINAuOIg"},"source":["MLKing_tokens = MLKing.split(\" \") "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s1Bgv8H5uOIh"},"source":["print(MLKing_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9BsbmFLGuOIh"},"source":["NLTK는 tokenization을 위한 함수를 제공한다.\n","\n","먼저, CoLab 환경에서 NLTK를 사용하기 위해 몇가지 작업을 수행할 필요가 있다. CoLab은 기본적으로 NLTK를 설치해 두고 있어 바로 `import nltk` 라는 명령어로 NLTK를 사용할 수 있지만 NLTK를 실행하는데 사용되는 여러 패키지들은 미리 설치해두고 있지 않다. 예를 들어 단어의 tokenization 을 위해서는 보통 `punkt`라는 패키지를 이용하는데 이 패키지를 사용하기 위해서는 매번 `punkt`를 다운로드해주어야 한다."]},{"cell_type":"code","metadata":{"id":"aaXcLcn0y_kG"},"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mj-272xNuOIi"},"source":["import nltk  \n","from nltk.tokenize import word_tokenize  \n","print(word_tokenize(MLKing))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G4CyxrsguOIj"},"source":["import nltk  \n","from nltk.tokenize import WordPunctTokenizer  \n","print(WordPunctTokenizer().tokenize(MLKing))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8c_Z93RSuOIk"},"source":["NLTK는 여러 토크나이저를 제공. 위의 두개의 코드는 word_tokenize 와 wordPunctTokenizer를 비교하고 있다. 거의 비슷해 보이지만 약간의 차이가 있다. 다음 문장을 통해 두개를 비교해 보자.\n"]},{"cell_type":"code","metadata":{"id":"d8-1v1efuOIk"},"source":["text = \"Don't be fooled by the looks.  Mr. Jake's smile is a fake.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LRb7szrjuOIk"},"source":["print(word_tokenize(text))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IO-HL5XzuOIl"},"source":["print(WordPunctTokenizer().tokenize(text))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDTS-H2muOIl"},"source":["WordPunctTokenizer는 구두점을 별도로 처리한다. 따라서 `'s -> ' , s` 로 분리가 된다."]},{"cell_type":"markdown","metadata":{"id":"_0cQGJWfuOIl"},"source":["### Tokenization 의 유의점.\n","\n","* `' . , ` 등의 구둣점, 쉼표 등을 지울 때 유의해야 함. 예) $24.95, 1,000,000.00 등과 같이 값을 표현할 때 마침표나 쉽표가 사용될 수 있는데 이 경우 단순히 구둣점이나 쉼표를 지우면 안됨.\n","* 단어 내에 띄어쓰기가 있는 경우도 있음. 예) New York (별도의 단어 사전을 통해 필터링해두어야)"]},{"cell_type":"markdown","source":["한글 문장의 토큰화도 스페이스(whitespace)를 이용하여 만들 수 있다. 보다 정교한 토큰화는 이후 Konlpy 를 이용하도록 하자."],"metadata":{"id":"a1KnknVbDl8-"}},{"cell_type":"code","source":["k_sentence = \"꿈을 이루고자 하는 용기만 있다면 모든 꿈을 이룰 수 있다\""],"metadata":{"id":"tJqX8NO4DleQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["k_tokens = k_sentence.split(\" \")\n","k_tokens"],"metadata":{"id":"ArUJEs0tD7of"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["그러나 단순히 스페이스를 바탕으로 만든 토큰은 분석에 필요하지 않은 단어가 포함되어 있거나 `이루고자`,   `이룰`과 같이 의미가 같지만 형태가 다른 단어가 존재하기 때문에 분석하기 어렵다. 따라서 단순히 공백을 중심으로 단어의 토큰을 만들기보다는 어간 추출이나 형태소 분석과 같은 표준화된 방법에 의해 토큰을 생성하는 것이 일반적이다. 형태소 분석을 하게 되면 `꿈을`과 같은 단어를 명사와 조사로 분리할 수도 있다."],"metadata":{"id":"HPOAm3OOEIhu"}},{"cell_type":"markdown","metadata":{"id":"srjUw-vMuOIm"},"source":["## 2.2 단어의 태깅\n","\n","토큰화된 단어의 품사를 태깅한다. 태깅된 품사는 단어의 필터링에 사용한다. 예를 들어 텍스트의 감정을 분석하기 위해서는 형용사 등의 단어를 필터링하고, 텍스트의 핵심단어를 추출하기 위해서는 명사 등의 단어를 필터링하여 분석한다."]},{"cell_type":"code","metadata":{"id":"DrAFI9cVuOIm"},"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.tag import pos_tag"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vQ0g59-KuOIn"},"source":["MLKing = \"\"\"I have a dream that my four little children will one day live in a nation \n","            where they will not be judged by the color of their skin \n","            but by the content of their character.\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"06Zxybs1uOIn"},"source":["MLKing_tokens = word_tokenize(MLKing)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5lYOqYImuOIn"},"source":["print(MLKing_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ph_sgUt_uOIn"},"source":["MLKing_tokens_tagged = pos_tag(MLKing_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y8orFYJruOIo"},"source":["print(MLKing_tokens_tagged)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zYV6yZq_uOIo"},"source":["## 2.3 데이터 클리닝(Data Cleaning) \n","\n","텍스트에서 노이즈를 제거\u001d한다. 노이즈의 예로는 대표적으로 불용어 (stopwords), 등장하는 빈도가 낮은 단어, 길이가 짧은 단어 등이 있다."]},{"cell_type":"markdown","metadata":{"id":"WgG2H5BvuOIo"},"source":["### 불용어 (Stopwords) 제거 \n","\n","NLTK에는 stopwords 리스트를 제공하고 있다. 그러나 기본적으로 제공되는 stopwords 리스트는 종종 부족하다. 이런 경우 기존의 리스트에 stopwords를 추가해서 사용한다. "]},{"cell_type":"code","metadata":{"id":"JNf3YWa7uOIo"},"source":["import nltk\n","from nltk.corpus import stopwords"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l0hiAti-uOIo"},"source":["print(stopwords.words('english')) # stopwords 리스트 확인"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oFkQfCAGuOIp"},"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","frank_text = \"\"\"\n","Six years have passed since I resolved on my present undertaking.\n","I can, even now, remember the hour from which I dedicated myself to\n","this great enterprise.  I commenced by inuring my body to hardship.\n","I accompanied the whale-fishers on several expeditions to the North Sea;\n","I voluntarily endured cold, famine, thirst, and want of sleep;\n","I often worked harder than the common sailors during the day and devoted\n","my nights to the study of mathematics, the theory of medicine,\n","and those branches of physical science from which a naval\n","adventurer might derive the greatest practical advantage.\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_q_-UDOGuOIp"},"source":["frank_text_tokens = word_tokenize(frank_text)\n","print(frank_text_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"skFqt4miuOIq"},"source":["stop_words = set(stopwords.words('english'))\n","filtered_tokens = [word for word in frank_text_tokens if word not in stop_words]\n","print(filtered_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yxbvZFeAuOIq"},"source":["### Update Stopwords"]},{"cell_type":"code","metadata":{"id":"iAGnCPEeuOIq"},"source":["stop_words.update(\",\", \".\", \";\", \"I\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BeB6Ye6muOIq"},"source":["filtered_tokens = [word for word in frank_text_tokens if word not in stop_words]\n","print(filtered_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8BDv8oKSuOIq"},"source":["그런데 이미 stopwords 리스트에 소문자 i 가 추가되어 있다. 하지만 파이썬이 불용어 처리를 할때 대소문자를 구별하기 때문에 I 가 제거되지 못했다. 따라서, 불용어 처리할 때 대소문자를 따로 불용어 리스트에 추가하기 보다는 텍스트를 소문자로 변환한 후에 처리하는 것이 좋은 방법이다. (Normalize)"]},{"cell_type":"markdown","metadata":{"id":"mz5-e4RZuOIq"},"source":["### 정규화 (Normalize)"]},{"cell_type":"markdown","metadata":{"id":"U2FuVI2guOIq"},"source":["정규화는 서로 다르게 표현된 단어를 같은 단어로 인식할 수 있게 변환하는 과정.\n","\n","* 대소문자 통합: `I & i -> .lower()`\n","* 어근 추출: lemmatization & stemming `see, saw, seen -> see`"]},{"cell_type":"code","metadata":{"id":"7jo8TrWEuOIr"},"source":["frank_text = \"\"\"\n","Six years have passed since I resolved on my present undertaking.\n","I can, even now, remember the hour from which I dedicated myself to\n","this great enterprise.  I commenced by inuring my body to hardship.\n","I accompanied the whale-fishers on several expeditions to the North Sea;\n","I voluntarily endured cold, famine, thirst, and want of sleep;\n","I often worked harder than the common sailors during the day and devoted\n","my nights to the study of mathematics, the theory of medicine,\n","and those branches of physical science from which a naval\n","adventurer might derive the greatest practical advantage.\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhql1YihuOIr"},"source":["frank_text_lower = frank_text.lower()\n","print(frank_text_lower)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sMNAaBpsuOIr"},"source":["frank_text_lower_tokens = word_tokenize(frank_text_lower)\n","print(frank_text_lower_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I0vCNNLquOIr"},"source":["stop_words = set(stopwords.words('english'))\n","filtered_lower_tokens = [word for word in frank_text_lower_tokens \n","                         if word not in stop_words]\n","print(filtered_lower_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["같은 의미를 가지고 있으나 표기가 다른 단어들을 통합하는 과정에서 주의할 점이 있다. 예컨대 United States와 US, USA는 같은 뜻으로 사용되지만 자연어처리 과정에서 이 세 개의 단어는 서로 다른 단어로 간주된다. 따라서 이런 경우 정규화 작업을 통해 세 개 단어의 표기방법을 통일해 줄 필요가 있다. 영어에서 대소문자의 구분은 또 다른 문제를 야기한다. 예를 들면 text, Text, TEXT가 서로 다른 단어로 간주된다. 따라서 대부분의 분석과정에서는 모든 텍스트를 소문자로 변환하는 것이 일반적이다. 그러나 소문자로 변환하는 것이 모든 문제의 해결책은 아니다. `우리'를 뜻하는 `us'와 `미국'을 뜻하는 `US'는 서로 다르기 때문이다. 모든 텍스트를 소문자로 변환한다면 두 개의 단어는 통합이 될 것이고 분석 결과에 영향을 미칠 수 있다. "],"metadata":{"id":"4ZjNAMx2HWJi"}},{"cell_type":"markdown","source":["### 어간추출\n","같은 의미의 단어를 통합하는 과정에서 주로 많이 사용하는 방법은 단어의 원형을 추출하는 것이다. 영어의 경우, 어간 추출(stemming) 혹은 표제어 추출(Lemmatization)을 이용하는 반면 한글은 형태소 분석이 주로 이용된다. 예를 들어 영어의 helps, helped, helping과 같은 단어는 같은 의미를 가진 단어지만 컴퓨터는 이를 같은 단어로 이해하지 못한다. 따라서 어간 추출 기법 등을 통해 단어의 원형인 help를 추출하여 세 개의 단어를 통합하는 과정이 필요하다."],"metadata":{"id":"6qbvbPWUIEy3"}},{"cell_type":"markdown","metadata":{"id":"AyyTiHRRuOIr"},"source":["### Lemmatization\n","\n","Lemmatization 은 단어의 표제어를 찾는 것을 의미한다. `dies, watched, has` 등은 각각 `die, watch, have`라는 기본형으로부터 파생되었다. Lemma는 다음과 같이 작동한다. "]},{"cell_type":"code","metadata":{"id":"O3KLpoB2uOIs"},"source":["import nltk\n","from nltk.stem import WordNetLemmatizer\n","lemma = WordNetLemmatizer()\n","words = ['dies', 'watched', 'has']\n","[lemma.lemmatize(w, 'v') for w in words] # 이들 단어가 동사로 쓰였다는 사실을 알려주기 위해 'v' 옵션추가"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ct7gVrO-uOIs"},"source":["### Stemming\n","\n","Stemming 은 어근을 추출하는 작업을 말한다. 표제어를 찾는 Lemma와는 어근을 찾기 때문에 조금 이상하게 보일 수도 있으나 실제 출현한 단어를 바탕으로 사전을 만든다는 관점에서는 Lemma보다 유용한 측면이 있다."]},{"cell_type":"code","metadata":{"id":"w6JU8dnWuOIs"},"source":["import nltk\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","stem = PorterStemmer()\n","words = ['dies', 'watched', 'has']\n","[stem.stem(w) for w in words]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xjKEJ07juOIs"},"source":["* Lemmatization 과 Stemming 은 비슷한 작업처럼 보이나 서로 다른 결과를 보여준다. 다음 시간에 두개의 차이를 좀더 살펴보겠다."]},{"cell_type":"markdown","metadata":{"id":"vA8FPdoAuOIs"},"source":["### 한글 정규화\n","\n","한글은 형태소 분석과정을 통해 데이터를 정규화\u001d한다. 형태소분석은 주로 조사 등을 제거하고 동사/형용사 등을 기본형으로 추출한다. 파이썬에서 주로 사용되는 한글 자연어처리 라이브러리는 'konlpy'이다. 다음의 명령어로 Colab에 'konlpy'를 설치하도록 하자.\n","\n","https://konlpy.org/en/latest/\n"]},{"cell_type":"code","metadata":{"id":"PYfLJyRC2r7h"},"source":["!pip install konlpy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oW0OXszfuOIs"},"source":["from konlpy.tag import Komoran\n","komoran = Komoran()\n","print(komoran.morphs('코모란 형태소 분석기를 사용하면 품사별로 단어를 추출할 수 있어요.'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VclEvzoxuOIs"},"source":["from konlpy.tag import Okt\n","okt = Okt()\n","print(okt.morphs('인터넷에서 쓰이는 용어도 분석이 되나욬ㅋㅋ', norm=True))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"euf2X1tHuOIt"},"source":["## 2.4 One-hot encoding\n","\n","One-hot encoding은 문자열을 숫자 벡터로 치환하는 방법이다. 단어 집합의 크기를 가진 벡터를 만들고, 표현하고 싶은 단어의 인덱스에 1을 부여하고, 그렇지 않은 인덱스에는 0을 부여하는 방법이다.\n","\n","문장: \"파이썬 자연어 처리 수업 시간\"\n","\n","생성된 테이블:\n","\n","| 0      | 1      | 2    | 3    | 4    |\n","|--------|--------|------|------|------|\n","| 파이썬 | 자연어 | 처리 | 수업 | 시간 |\n","\n","```\n","* 파이썬 -> [1, 0, 0, 0, 0]\n","* 처리 -> [0, 0, 1, 0, 0]\n","```"]},{"cell_type":"code","metadata":{"id":"eDKL3bD2uOIt"},"source":["word_token = ['파이썬', '자연어', '처리', '수업', '시간']\n","word2index = {}\n","for word in word_token:\n","    if word not in word2index.keys():\n","        word2index[word] = len(word2index)\n","print(word2index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cSAK15cruOIt"},"source":["def one_hot_encoding(word, word2index):\n","    one_hot_vector = [0]*(len(word2index))\n","    index = word2index[word]\n","    one_hot_vector[index] = 1\n","    return one_hot_vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H7Z3LiM7uOIt"},"source":["one_hot_encoding(\"파이썬\", word2index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZJMvFBTtuOIt"},"source":["one_hot_encoding(\"처리\", word2index)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6O6ixdV2uOIt"},"source":["이렇게 벡터화된 단어의 리스트는 다양한 연산을 할 수 있다. 예를 들어 단어의 공출현 빈도를 계산하려면 간단히 단어 벡터의 내적을 구하면 \u001d된다. 그러나, 단어가 많아질수록 벡터의 크기가 커진다는 한계점이 있어 더 많은 연산이 필요하다. 또한 벡터 표현만으로는 단어의 연관성을 파악할 수 없다는 문제점이 있다. 고양이 [0,0,1,0,0] 와 강아지 [1,0,0,0,0]의 유사성을 벡터만으로 판단할 수 없기 때문이다. 그래서 최근에는 이런 문제를 해결하기 위해 신경망 기반의 워드임베딩(word embedding)모델을 사용한다."]},{"cell_type":"code","source":[" "],"metadata":{"id":"hnxgABsaH0dg"},"execution_count":null,"outputs":[]}]}